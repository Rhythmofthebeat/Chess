{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rhythmofthebeat/Chess/blob/main/Student_NLPForFinance_Section3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGUbqvTdfjdo"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVuaBWBN6OBW"
      },
      "source": [
        "# **Text Classification using BERT**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asjEyQW56LAy"
      },
      "source": [
        "<img src=\"https://www.codemotion.com/magazine/wp-content/uploads/2020/05/bert-google.png\" width=800>\n",
        "\n",
        "BERT stands for Bidirectional Encoder Representations from Transformers. It is an advanced, pre-trained NLP model that understands language by looking at the context of each word. Because BERT has already been created and pre-trained for us (by Google), it can be fine-tuned for our finance task by adding just one additional output layerâ€”just like that, we can create a state-of-the-art model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54TzPSQ8LETS"
      },
      "source": [
        "## Goals\n",
        "In this notebook we will be:\n",
        "\n",
        "*   Learning and understanding the BERT ML technique and its signficance in NLP\n",
        "*   Preprocessing data for BERT and training our model\n",
        "*   Running and finetuning our BERT model with the *PyTorch huggingface* transformer library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoJj6q6UKona"
      },
      "source": [
        "<font color=\"#de3023\"><h5><b>**NOTE:** We'll need to use a GPU for this notebook. Go to \"Runtime\" -> \"Change runtime type\" -> and set Hardware Accelerator to \"GPU.\"</b></h5></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwYCqM646X2u"
      },
      "source": [
        "# **Milestone 1: Learning and understanding the BERT ML technique and its significance in NLP.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbeROSdhx_nI"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "!pip install transformers\n",
        "import os\n",
        "import gdown\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn import metrics\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# gdown.download('https://drive.google.com/uc?id=1q4U2gVY9tWEPdT6W-pdQpKmo152QqWLE', 'finance_train.csv', True)\n",
        "# gdown.download('https://drive.google.com/uc?id=1nIBqAsItwVEGVayYTgvybz7HeK0asom0', 'finance_test.csv', True)\n",
        "\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_test.csv'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_train.csv'\n",
        "\n",
        "def get_finance_train():\n",
        "  df_train = pd.read_csv(\"finance_train.csv\")\n",
        "  return df_train\n",
        "def get_finance_test():\n",
        "  df_test = pd.read_csv(\"finance_test.csv\")\n",
        "  return df_test\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "print (\"Train and Test Files Loaded as train.csv and test.csv\")\n",
        "\n",
        "LABEL_MAP = {0 : \"negative\", 1 : \"neutral\", 2 : \"positive\"}\n",
        "NONE = 4 * [None]\n",
        "RND_SEED=2020\n",
        "\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = [\"Negative\",\"Neutral\",\"Positive\"]\n",
        "  df_cm = pd.DataFrame(cm,index =labels,columns = labels)\n",
        "  fig = plt.figure(figsize=(14,12))\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZehuM8rNNdef"
      },
      "source": [
        "## Let's Talk about Bert!\n",
        "\n",
        "As described above, BERT is a pretrained NLP model that is more contextually aware than anything we've seen before. Specifically, it is an NLP model designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on what precedes and succeeds a word. Therefore, it is better able to capture a word within the context of a sentence. Some other important highlights about BERT are signified below:\n",
        "\n",
        "1. **Quick Training:**  BERT comes pre-trained, so we can train it on our data very quickly.\n",
        "\n",
        "2. **Less Data:** Because BERT is pre-trained, we don't need as much training data.\n",
        "\n",
        "3. **Great Results:** BERT has proven to be an excellent NLP model.\n",
        "\n",
        "4. **Multi-Lingual Capabilities:** If we want to apply our network to foreign markets, BERT works on more than just English!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8dkQmKyO2uU"
      },
      "source": [
        "Please take 5 minutes to watch this informative video on the BERT model and its high-level significance and architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPLzjwBvNIli",
        "cellView": "form"
      },
      "source": [
        "#@title BERT Summary Video\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"854\" height=\"480\" src=\"https://www.youtube.com/embed/zMxvS7hD-Ug\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSDKzmXLOO_8",
        "cellView": "form"
      },
      "source": [
        "#@title Exercise. What are some interesting NLP concepts or unique aspects of the BERT model you learned in the video?\n",
        "_1_ = '' #@param {type:\"string\"}\n",
        "_2_ = '' #@param {type:\"string\"}\n",
        "_3_ = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU05R8r5ObJm"
      },
      "source": [
        "#### **Discussion Exercise: Advantages and disadvantages of BERT**\n",
        "\n",
        "What do you think are some possible benefits and weaknesses to using a pre-trained model such as BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cFNUTzRPCXQ"
      },
      "source": [
        "# **Milestone 2: Preprocessing data for BERT and training our model.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACBeiS6GPPYk"
      },
      "source": [
        "## **Reading in the Datasets**\n",
        "\n",
        "Just as in previous notebooks, use the `get_finance_train()` and `get_finance_test()` methods to get the train and test datasets and save them in variables named `df_train` and `df_test` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwAbeEoKPit1"
      },
      "source": [
        "df_train = None ### YOUR CODE HERE\n",
        "df_test = None ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W-poJDNSRVL"
      },
      "source": [
        "For simplicity, save sentences and labels columns of `df_train` in variables named `sentences` and `labels` respectively.\n",
        "\n",
        "\n",
        "**Note: To access a column from a dataframe based on the column name, you can use the code structure: `column = df_train[\"column_name\"].values`. Replace `column_name` with your desired column to access it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oQiI3g6AJ-d"
      },
      "source": [
        "sentences = None ### YOUR CODE HERE\n",
        "labels = None ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NXZ7Fw1AmOr"
      },
      "source": [
        "## **Tokenization & Input Formatting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lxvV54ZAuN4"
      },
      "source": [
        "Just as before, we need to break down our input sentences to smaller tokens. BERT expects input sentences to be broken down into individual tokens and the input data needs to follow BERT's pre-defined format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGEtAsHJBfBo"
      },
      "source": [
        "### BERT Tokenizer ###\n",
        "\n",
        "The BERT Tokenizer is a two-step process in processing our data. First, we will initialize the tokenizer using our dataset. Thereafter, we will convert the original sentences to tokenized sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-aDHt-_VauL"
      },
      "source": [
        "#### Step 1 : Create the Tokenizer ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axi-xi5ZBqjm"
      },
      "source": [
        "#### **Coding Exercise: Tokenize with Bert**\n",
        "\n",
        "When using BERT, our text needs to be split into tokens and those tokens need to be mapped to an index using BERT's vocabulary. We will use the `BertTokenizer` module from the transformer library to tokenize our data. To do so, call the function `BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case = True)` and save the returned value in a variable named `tokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vOrBr3cRzHs"
      },
      "source": [
        "tokenizer = None ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roUE0jNwTUjt"
      },
      "source": [
        "#### **Coding Exercise: Vocabulary Size**\n",
        "\n",
        "The BERT Tokenizer object you created keeps track of the size of the vocabulary we are dealing with in this dataset. Find and print the number of unique tokens in the BERT model by calling `tokenizer.vocab_size`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQVK-3jNTmtj"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsCBSEwTUFDF"
      },
      "source": [
        "#### **Coding Exercise: Applying the tokenizer**\n",
        "\n",
        "Apply the tokenizer to the first element in your training dataset: `sentences[0]`. To do so, call the `tokenizer.tokenize(SENTENCE)` function while passing in your sentence in place of `SENTENCE` as shown. Print the original sentence relative to the tokenized sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVoeUVNcUjNu"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzsYubUFU_uJ"
      },
      "source": [
        "#### **Discussion Exercise: BERT Tokens**\n",
        "\n",
        "What do you think about the BERT generated tokens? What do think is the significance of some of the stranger tokens and the # characters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOk0VvHAEFr9"
      },
      "source": [
        "#### Step 2 : Map the tokens to our index ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0l2OHikEX8a"
      },
      "source": [
        "Each token has a corresponding index in the vocabulary. To keep track of the vector representations more concisely, we will convert each of the tokenized lists into a list of indices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyrDHfCjh9by"
      },
      "source": [
        "#### **Coding Exercise: Map tokens to indices**\n",
        "\n",
        "Map the tokens to indices on the first element in your training dataset: `sentences[0]`. To do so,  use the `convert_tokens_to_ids()` function of the `tokenizer` to encode our word tokens into numerical values. Call the command `tokenizer.convert_tokens_to_ids(TOKENIZED_SENTENCE)` passing in the tokenized version of `sentences[0]` that you created above for `TOKENIZED_SENTENCE`. Print the tokenized sentence relative to the mapped indices sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Fm8BD3EWHJ"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMu6yub9CaSh"
      },
      "source": [
        "### Text Formatting ###\n",
        "\n",
        "When using BERT, we need to modify our input to match the BERT Format. To do so, we will apply these three steps on the input sentences:\n",
        "\n",
        "1. Add special tokens to the start (`[CLS]`) and end (`[SEP]`) of each sentence.\n",
        "2. Pad & truncate all sentences to a single constant length.\n",
        "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
        "\n",
        "Take note of the visualization on **Sentence A** below that elucidates these steps. However, let us understand each of these steps in more detail before proceeding to apply them to our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMUEQzjxnLNK"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1sN5pvRjF1PnV72AXu9d8DLVDllAYOBzk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0gZApxsGu-e"
      },
      "source": [
        "#### **Special Tokens** ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j6ihrw7G6bf"
      },
      "source": [
        "These are two tokens we add to the start and end respectively:\n",
        "\n",
        "\n",
        "\n",
        "*   `[CLS]` - stands for \"classification,\" and is used to identify new sentences.\n",
        "*   `[SEP]` - stands for \"separator,\" and identifies if a pair of sentences are consecutive in a corpus or not (used for next sentence prediction).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y2QFwsqot32"
      },
      "source": [
        "#### **Coding Exercise: Add special tokens**\n",
        "\n",
        "We want to add these special tokens to the start and end of each one of our preprocessed sentences. To do so, fill out the code below to loop through your `sentences` array and create a new sentence that adds `\"[CLS] \"` to the start of the sentence and that adds `\" [SEP]\"` to the end of the sentence. Add the newly modified sentence to the list variable named `sentences_with_special_tokens`.\n",
        "\n",
        "**Note: When you add the `\"[CLS] \"` string to the front of the sentence, ensure that you add it with the space character at the end as shown. Similarly, add the `\" [SEP]\"` string to the end of the sentence with a space character at the start as shown.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0UZQC9KpOAn"
      },
      "source": [
        "sentences_with_special_tokens = []\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1rAcuirN68"
      },
      "source": [
        "#### **Coding Exercise: Tokenize your sentences**\n",
        "\n",
        "Now we will tokenize our new list of input sentences using our BERT `tokenizer`. Do so by looping through your new `sentences_with_special_tokens` list and apply `tokenizer.tokenize(SENTENCE)` to each sentence. Add each tokenized sentence to a list named `tokenized_texts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wn28oGGrpgF"
      },
      "source": [
        "tokenized_texts = []\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X47WRWKnJaje"
      },
      "source": [
        "#### **Sentence Length** ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt0BAL9BJjBT"
      },
      "source": [
        "Since our input has sentences of varying length, we need to pad them to be the same length. BERT has the follow constraints on sentence length:\n",
        "1. All sentences must be padded or truncated to a single, fixed length.\n",
        "2. The maximum sentence length is 512 tokens.\n",
        "\n",
        "In order to handle this, every input sequence is padded to pre-defined fixed length with a special `[PAD]` token, which pads the sequence with zeros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e2jJaPescCj"
      },
      "source": [
        "#### **Coding Exercise: Encode tokenized sentences with indices**\n",
        "\n",
        "Before we properly pad the sentences, we will first map each token to its corresponding index for each tokenized sentence. To do so, we will loop through your `tokenized_texts` variable call the `tokenizer.convert_tokens_to_ids(TOKENIZED_SENTENCE)` while passing in your tokenized sentence to convert it to a list of corresponding indices. Add each converted list to the variable named `input_ids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_deSpjHtJ_8"
      },
      "source": [
        "input_ids = []\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFgUi0G-tma-"
      },
      "source": [
        "#### **Coding Exercise: Padding your input**\n",
        "\n",
        "Now we will pad our input to ensure that every sequence has the same length. To do so, we will utilize the `keras` function named `pad_sequences()`. Call the function below to properly pad your sequences. We want to pad every sequence to a length of `128`, so ensure that you pass in `128` for `maxlen` in the function call. Print the first element of your new `input_ids` to observe the new padded sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7hdDYWTtlOu"
      },
      "source": [
        "input_ids = pad_sequences(input_ids,\n",
        "                          maxlen=None, ### YOUR CODE HERE\n",
        "                          dtype=\"long\",\n",
        "                          truncating=\"post\",\n",
        "                          padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90eOsNIMKyZB"
      },
      "source": [
        "####**Attention Masks**####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_-H9HfbK4sh"
      },
      "source": [
        "An Attention Mask is an array of 1s and 0s indicating which tokens are padding and which are not. The idea behind attention masks is that we do not want the extra padded sequence tokens to contribute to the input features of the machine learning model. So, we essentially zero them out to highlight their insignificance before passing them through the model.\n",
        "\n",
        "Each sequence has a corresponding attention mask. Consider the input sequence below and its corresponding attention mask. We want to create an attention mask for each sequence before passing our data through the BERT model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WysesWTwwrka"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1LgtkmZU8fQZsn9L85lENHgGxej3i61TP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuW6eulYvHjc"
      },
      "source": [
        "#### **Coding Exercise: Create attention masks**\n",
        "\n",
        "Create an attention mask for each sequence in your `input_ids` list. The attention mask for a particular tokenized sequence will have the same exact length and will have a `1.0` at every index that there is a token and a `0.0` at every index that there is a padding. Loop through your `input_ids` and for each sequence create a corresponding attention mask and add it to the `attention_masks` list.\n",
        "\n",
        "**Hint: To get the attention mask given a tokenized sequence, you may use the line of code `mask=[float(i>0) for i in SEQUENCE]` where `SEQUENCE` is an element of your `input_ids` list.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoLBbuUgSJEU"
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "###YOUR CODE HERE###\n",
        "\n",
        "\n",
        "###END CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh5_xaQ8YRdb"
      },
      "source": [
        "## **Setting up your Data for BERT**\n",
        "\n",
        "Finally, we need to split our data and convert it to objects that will work most efficiently with the BERT model training procedure. Specifically, we will split our training data into training and validation sets. Additionally, we will convert our data objects into `tensor` s such that we can easily feed the input into the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQR_H7xAyFlX"
      },
      "source": [
        "#### **Coding Exercise: Train/Test Split**\n",
        "\n",
        "To run machine learning models, we first need to split our dataset into training and validation sections. We can do so by using the `train_test_split()` function. One call to this function returns four separate items:\n",
        "  1. X_train\n",
        "  2. X_val\n",
        "  3. y_train\n",
        "  4. y_val\n",
        "\n",
        "We will save these returned elements in variables with respective names. Call the `train_test_split()` function, passing in `input_ids`, `labels`, `test_size=0.15`, and `random_state=RND_SEED`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x45mr_TJyP5J"
      },
      "source": [
        "X_train, X_val, y_train, y_val = NONE ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZOG4dG1zA3X"
      },
      "source": [
        "We also want to split our attention masks in the same way. Similarly, call the `train_test_split()` function, passing in `attention_masks`, `input_ids`, `test_size=0.15`, and `random_state=RND_SEED`. However, we only need to save the first two returned items in variables named `train_masks` and `validation_masks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzU-N_UxzC2n"
      },
      "source": [
        "train_masks, validation_masks, _, _ = NONE ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJUPm4lB5Hom"
      },
      "source": [
        "In addition to converting our data to `tensor`, `tensorflow` array objects, we will also create a `DataLoader` for our input. A `DataLoader` is simply an object that simplifies and streamlines feeding in data to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm8uhq7rU3nY",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to convert data to tensors and create DataLoaders\n",
        "train_inputs = torch.tensor(np.array(X_train));\n",
        "validation_inputs = torch.tensor(np.array(X_val));\n",
        "train_masks = torch.tensor(np.array(train_masks));\n",
        "validation_masks = torch.tensor(np.array(validation_masks));\n",
        "train_labels = torch.tensor(np.array(y_train));\n",
        "validation_labels = torch.tensor(np.array(y_val));\n",
        "\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels);\n",
        "train_sampler = RandomSampler(train_data); # Samples data randonly for training\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size);\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels);\n",
        "validation_sampler = SequentialSampler(validation_data); # Samples data sequentially\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73xKQg_qWDq1"
      },
      "source": [
        "# **Milestone 3: Running and finetuning our BERT model with the PyTorch huggingface transformer library.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnAnFaYTY4y4"
      },
      "source": [
        "To use BERT in our sentiment analysis, we first want to modify BERT to output either a positive, neutral, or negative classification. Next,  we want to train the model on our dataset so that the entire model, end-to-end, is well-suited to our task.\n",
        "\n",
        "The huggingface PyTorch implementation includes a set of interfaces designed for a variety of NLP tasks. These interfaces are all built on top of a pre-trained BERT model, and each has different top layers and output types designed to accomodate their specific NLP task. (An analogy: imagine BERT is a garden hose; then the huggingface PyTorch interfaces are like different nozzles for the garden hose, so we can use it for \"jet,\" \"mist,\" etc.)\n",
        "\n",
        "For our sentiment classification task we will use:\n",
        "\n",
        "* **BertForSequenceClassification** - This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "\n",
        "The documentation for this can be found [here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frAYD51x54gW"
      },
      "source": [
        "We will create a `BertForSequenceClassification` model and save it in a variable named `model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-iu1Nm0yiTS",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to initialize your BertForSequenceClassification model\n",
        "#Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT small model, with an uncased vocab.\n",
        "    num_labels = 3,\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ");\n",
        "\n",
        "# Given that this a huge neural network, we need to explicity specify\n",
        "# in pytorch to run this model on the GPU.\n",
        "model.cuda();\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNBo_lQ0a2D"
      },
      "source": [
        "#### **Coding Exercise: Initialize hyperparameters**\n",
        "\n",
        "There are two hyperparamters we will consider: **Learning rate** and **Epochs**. Improperly setting these hyperparamters could lead to overfitting or underfitting. We will set these values as follows:\n",
        "\n",
        "1. **Learning rate**: 2e-5\n",
        "2. **Epochs**: 4\n",
        "\n",
        "The learning rate parameter comes as part of an optimizer object that we use in our model. Only change the value of the `lr` variable inside the `AdamW` object. Fill in these hyperparameter values below and run the code cell before proceeding. Revisit this cell after you complete the notebook to try and improve your model through modifying these parameter values!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Toc3r89-76M8"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, ### YOUR CODE HERE\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "epochs = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP1Ltw979KEa"
      },
      "source": [
        "#### **Discussion Exercise: Training Model Steps**\n",
        "\n",
        "Below are a list of steps taken to train the model listed in no particular order. Look at the steps and discuss the significance of each. Afterwards, order the steps based on how you think the model training procedure is carried out and list them in order below.\n",
        "\n",
        "Training Steps:\n",
        "*   Backward pass (backpropagation)\n",
        "*   Clear out the gradients calculated in the previous pass\n",
        "*   Update parameters\n",
        "*   Unpack our data inputs and labels from the DataLoader objects\n",
        "*   Forward pass (feed data through network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yuf46Ae8-Lvf"
      },
      "source": [
        "#@title Exercise. Ordered Model Training Steps\n",
        "\n",
        "_1_ = '' #@param {type:\"string\"}\n",
        "_2_ = '' #@param {type:\"string\"}\n",
        "_3_ = '' #@param {type:\"string\"}\n",
        "_4_ = '' #@param {type:\"string\"}\n",
        "_5_ = '' #@param {type:\"string\"}\n",
        "\n",
        "print('{}\\n{}\\n{}\\n{}\\n{}'.format(\n",
        "    '1. Unpack our data inputs and labels from the DataLoader objects',\n",
        "    '2. Clear out the gradients calculated in the previous pass',\n",
        "    '3. Forward pass',\n",
        "    '4. Backward pass',\n",
        "    '5. Update hyperparamters'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpNhCuKZ33X8",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to train your model!\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# We'll store training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "training_stats = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Training\n",
        "    print('Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training the model')\n",
        "    # Reset the total loss for  epoch.\n",
        "    total_train_loss = 0\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "\n",
        "        # STEP 1 & 2: Unpack this training batch from our dataloader.\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # STEP 3\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # STEP 4\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # It returns the loss (because we provided labels) and\n",
        "        # the \"logits\"--the model outputs prior to activation.\n",
        "        outputs = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # STEP 5\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # STEP 6\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    # Validation\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"Evaluating on Validation Set\")\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        #Step 1 and Step 2\n",
        "        # Unpack this validation batch from our dataloader.\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "\n",
        "    training_loss.append(avg_train_loss)\n",
        "    validation_loss.append(avg_val_loss)\n",
        "    # Record all statistics from this epoch.\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy\n",
        "\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJeeXKQe_Qkm"
      },
      "source": [
        "#### **Coding Exercise: Plot your losses!**\n",
        "\n",
        "When we trained the model above, the training and validation losses were saved in variables named `training_loss` and `validation_loss` respectively. Let us plot these curves and analyze what we see!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L7K8cTM_ili"
      },
      "source": [
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.title('Loss over Time')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQD-f1dOAiTT"
      },
      "source": [
        "#### **Coding Exercise: Evaluate your test set accuracy.**\n",
        "\n",
        "To be able to assess the accuracy on the test set, we need to carry out the data preprocessing, tokenization, padding, and formatting on the test set saved in `df_test` from earlier.\n",
        "\n",
        "First, to make this process easier, save the sentences and labels from `df_test` in variables named `test_sentences` and `test_labels`.\n",
        "\n",
        "**Note: To access a column from a dataframe based on the column name, you can use the code structure: `column = df_test[\"column_name\"].values`. Replace `column_name` with your desired column to access it.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01aUl-mxBLgX"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hACjYuF2AJo9"
      },
      "source": [
        "Now we will format our test input data similarly to how we formatted our training input data. Run the cell below to create the variables `test_input_ids` and `test_attention_masks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ncZome9-tPE",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to process and prepare our test data!\n",
        "test_input_ids, test_attention_masks = [], []\n",
        "\n",
        "# Add Special Tokens\n",
        "test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_test_sentences = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
        "\n",
        "# Encode Tokens to Word IDs\n",
        "test_input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_test_sentences]\n",
        "\n",
        "# Pad the inputs\n",
        "test_input_ids = pad_sequences(test_input_ids,\n",
        "                               maxlen=128,\n",
        "                               dtype=\"long\",\n",
        "                               truncating=\"post\",\n",
        "                               padding=\"post\")\n",
        "\n",
        "# Create Attention Masks\n",
        "for sequence in test_input_ids:\n",
        "  mask = [float(i>0) for i in sequence]\n",
        "  test_attention_masks.append(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca5qUrf1Ci8m"
      },
      "source": [
        "Just as before, we will convert our data to `tensor` and create a `DataLoader` for our inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CiYwcQIANi8",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to convert data to tensors and create DataLoaders\n",
        "batch_size = 32\n",
        "test_input_ids = torch.tensor(test_input_ids)\n",
        "test_attention_masks = torch.tensor(test_attention_masks)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArwOSMuwAXzs"
      },
      "source": [
        "Finally, run the cell below to evaluate your accuracy on your test dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4rX-vG7AaXl",
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate Test Set Accuracy!\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "# Final tracking variables\n",
        "y_logits, y_true, y_preds = [], [], []\n",
        "\n",
        "# Gather logit predictions\n",
        "for chunk in predictions:\n",
        "  for logits in chunk:\n",
        "    y_logits.append(logits)\n",
        "\n",
        "# Gather true labels\n",
        "for chunk in true_labels:\n",
        "  for label in chunk:\n",
        "    y_true.append(label)\n",
        "\n",
        "# Gather real predictions\n",
        "for logits in y_logits:\n",
        "  y_preds.append(np.argmax(logits))\n",
        "\n",
        "print ('Test Accuracy: {:.2%}'.format(metrics.accuracy_score(y_preds,y_true)))\n",
        "plot_confusion_matrix(y_true,y_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-j-nUj_EDkK"
      },
      "source": [
        "#### **Discussion Exercise: Significance of your test accuracy**\n",
        "\n",
        "What do you think about the accuracy of your model? Do you think it would perform just as well on a larger dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvNbJi3hJJ-p"
      },
      "source": [
        "# Congratulations on completing your BERT Model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKUNqg3bJO86"
      },
      "source": [
        "## **Wrap-Up**\n",
        "\n",
        "In this notebook, you've:\n",
        "\n",
        " - Learned about the advanced BERT model\n",
        " - Learned to format and filter data for efficent model training and testing.\n",
        "\n",
        "Over these three notebooks, you have delved into the NLP + Finance and have gained subject matter knowledge that will allow you to devise intelligent financial-related models. You can take your newly acquired skills and apply them to other domains and datasets for several other tasks. For example, you may take your skills and utilize sentiment analysis to predict stock prices! The sky's the limit with your new-found skills and expertise!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKtr5xWpzflg"
      },
      "source": [
        "<img src=\"https://www.sr-sv.com/wp-content/uploads/2019/06/NLP_0000.jpg\" width=\"800\">"
      ]
    }
  ]
}